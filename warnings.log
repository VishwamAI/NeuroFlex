============================= test session starts ==============================
platform linux -- Python 3.9.16, pytest-8.3.3, pluggy-1.5.0
rootdir: /home/ubuntu/NeuroFlex
configfile: pytest.ini
collected 274 items

tests/advanced_models/test_advanced_math_solving.py .DEBUG: Expected calls: 20
DEBUG: Actual select_action calls: 20
DEBUG: Actual update calls: 20
..............
tests/advanced_models/test_advanced_time_series_analysis.py .....RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =            5     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f= -0.00000D+00    |proj g|=  1.03234D+15

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
    5      1     13      1     0     0   1.032D+15  -0.000D+00
  F =  -0.0000000000000000     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =            5     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f= -0.00000D+00    |proj g|=  1.03234D+15

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
    5      1     13      1     0     0   1.032D+15  -0.000D+00
  F =  -0.0000000000000000     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =            5     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f= -0.00000D+00    |proj g|=  9.76925D+14

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
    5      1     13      1     0     0   9.769D+14  -0.000D+00
  F =  -0.0000000000000000     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =            5     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f= -0.00000D+00    |proj g|=  9.76925D+14

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
    5      1     13      1     0     0   9.769D+14  -0.000D+00
  F =  -0.0000000000000000     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =            5     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f= -0.00000D+00    |proj g|=  9.76925D+14

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
    5      1     13      1     0     0   9.769D+14  -0.000D+00
  F =  -0.0000000000000000     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
............... N =            5     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f= -0.00000D+00    |proj g|=  9.76925D+14

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
    5      1     13      1     0     0   9.769D+14  -0.000D+00
  F =  -0.0000000000000000     

CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =            6     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f= -0.00000D+00    |proj g|=  2.09088D+15

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
    6      1     21      1     0     0   2.091D+15  -0.000D+00
  F =  -0.0000000000000000     

ABNORMAL_TERMINATION_IN_LNSRCH                              
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =            6     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f= -0.00000D+00    |proj g|=  2.09088D+15

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
    6      1     21      1     0     0   2.091D+15  -0.000D+00
  F =  -0.0000000000000000     

ABNORMAL_TERMINATION_IN_LNSRCH                              
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =            5     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f= -0.00000D+00    |proj g|=  2.07879D+15

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
    5      1     21      1     0     0   2.079D+15  -0.000D+00
  F =  -0.0000000000000000     

ABNORMAL_TERMINATION_IN_LNSRCH                              
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =            5     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f= -0.00000D+00    |proj g|=  2.07879D+15

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
    5      1     21      1     0     0   2.079D+15  -0.000D+00
  F =  -0.0000000000000000     

ABNORMAL_TERMINATION_IN_LNSRCH                              
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =            5     M =           10

At X0         0 variables are exactly at the bounds
....
At iterate    0    f= -0.00000D+00    |proj g|=  7.46977D+30

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
    5      1     21      1     0     0   7.470D+30  -0.000D+00
  F =  -0.0000000000000000     

ABNORMAL_TERMINATION_IN_LNSRCH                              
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =            5     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f= -0.00000D+00    |proj g|=  7.46977D+30

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
    5      1     21      1     0     0   7.470D+30  -0.000D+00
  F =  -0.0000000000000000     

ABNORMAL_TERMINATION_IN_LNSRCH                              
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =            5     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f= -0.00000D+00    |proj g|=  0.00000D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
    5      0      1      0     0     0   0.000D+00  -0.000D+00
  F =  -0.0000000000000000     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =            5     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f= -0.00000D+00    |proj g|=  0.00000D+00

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
    5      0      1      0     0     0   0.000D+00  -0.000D+00
  F =  -0.0000000000000000     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =            5     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f= -0.00000D+00    |proj g|=  5.74366D+14

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
    5      1     21      1     0     0   5.744D+14  -0.000D+00
  F =  -0.0000000000000000     

ABNORMAL_TERMINATION_IN_LNSRCH                              
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =            5     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f= -0.00000D+00    |proj g|=  5.74366D+14

           * * *
.
tests/advanced_models/test_multi_modal_learning.py ........Training: 1/1Training: 1/1Training: 1/1Training: 1/1Training: 1/1Training: 1/1Training: 1/1Training: 1/1Training: 1/1Training: 1/1Training: 1/1..
tests/advanced_models/test_neural_networks.py ..Training model...
Improving model performance...
Updating model...
Handling gradient explosion...
Attempting to escape local minimum...
New learning rate: 0.00125
..Training model...
Early stopping at epoch 19
.Training model...
.
tests/advanced_models/test_reinforcement_learning.py ......
tests/advanced_models/test_reinforcement_learning_advancements.py ..........
tests/ai_ethics/test_ai_ethics.py .....s
tests/ai_ethics/test_security_integration.py Setting up AlphaFold model with params: {'max_recycling': 3}
Preparing features for sequence: TY
Predicting protein structure
Calculating pLDDT scores
Calculating predicted aligned error
Preparing features for sequence: CM
Predicting protein structure
Calculating pLDDT scores
Calculating predicted aligned error
Dataset 'merged_bio_data' saved to /tmp/tmp4eg4hp1k/merged_bio_data.nc
Dataset loaded from /tmp/tmp4eg4hp1k/merged_bio_data.nc and registered as 'merged_bio_data'
.Setting up AlphaFold model with params: {'max_recycling': 3}
Preparing features for sequence: TY
Predicting protein structure
Calculating pLDDT scores
Calculating predicted aligned error
Preparing features for sequence: CM
Predicting protein structure
Calculating pLDDT scores
Calculating predicted aligned error
Dataset 'merged_bio_data' saved to /tmp/tmpm0lff9v9/merged_bio_data.nc
Dataset loaded from /tmp/tmpm0lff9v9/merged_bio_data.nc and registered as 'merged_bio_data'
.Setting up AlphaFold model with params: {'max_recycling': 3}
Preparing features for sequence: TY
Predicting protein structure
Calculating pLDDT scores
Calculating predicted aligned error
Preparing features for sequence: CM
Predicting protein structure
Calculating pLDDT scores
Calculating predicted aligned error
Dataset 'merged_bio_data' saved to /tmp/tmpbis_hvcc/merged_bio_data.nc
Dataset loaded from /tmp/tmpbis_hvcc/merged_bio_data.nc and registered as 'merged_bio_data'
.Setting up AlphaFold model with params: {'max_recycling': 3}
Preparing features for sequence: TY
Predicting protein structure
Calculating pLDDT scores
Calculating predicted aligned error
Preparing features for sequence: CM
Predicting protein structure
Calculating pLDDT scores
Calculating predicted aligned error
Dataset 'merged_bio_data' saved to /tmp/tmpdc7uggp6/merged_bio_data.nc
Dataset loaded from /tmp/tmpdc7uggp6/merged_bio_data.nc and registered as 'merged_bio_data'
.Setting up AlphaFold model with params: {'max_recycling': 3}
Preparing features for sequence: TY
Predicting protein structure
Calculating pLDDT scores
Calculating predicted aligned error
Preparing features for sequence: CM
Predicting protein structure
Calculating pLDDT scores
Calculating predicted aligned error
Dataset 'merged_bio_data' saved to /tmp/tmpj4wuf_xs/merged_bio_data.nc
Dataset loaded from /tmp/tmpj4wuf_xs/merged_bio_data.nc and registered as 'merged_bio_data'
Training model...
Epoch 1/10 - Average Loss: 2.3680
Epoch 1/10 - Fairness evaluation: {'disparate_impact': 0.9}
Epoch 2/10 - Average Loss: 2.2818
Epoch 2/10 - Fairness evaluation: {'disparate_impact': 0.9}
Epoch 3/10 - Average Loss: 2.3833
Epoch 3/10 - Fairness evaluation: {'disparate_impact': 0.9}
Epoch 4/10 - Average Loss: 2.2759
Epoch 4/10 - Fairness evaluation: {'disparate_impact': 0.9}
Epoch 5/10 - Average Loss: 2.2617
Epoch 5/10 - Fairness evaluation: {'disparate_impact': 0.9}
Epoch 6/10 - Average Loss: 2.3097
Epoch 6/10 - Fairness evaluation: {'disparate_impact': 0.9}
Epoch 7/10 - Average Loss: 2.2258
Epoch 7/10 - Fairness evaluation: {'disparate_impact': 0.9}
Epoch 8/10 - Average Loss: 2.2691
Epoch 8/10 - Fairness evaluation: {'disparate_impact': 0.9}
Epoch 9/10 - Average Loss: 2.2720
Epoch 9/10 - Fairness evaluation: {'disparate_impact': 0.9}
Epoch 10/10 - Average Loss: 2.2786
Epoch 10/10 - Fairness evaluation: {'disparate_impact': 0.9}
Final model health status: <MagicMock name='AdvancedSecurityAgent().check_model_health()' id='132339286172480'>
.Setting up AlphaFold model with params: {'max_recycling': 3}
Preparing features for sequence: TY
Predicting protein structure
Calculating pLDDT scores
Calculating predicted aligned error
Preparing features for sequence: CM
Predicting protein structure
Calculating pLDDT scores
Calculating predicted aligned error
Dataset 'merged_bio_data' saved to /tmp/tmpxg59md9s/merged_bio_data.nc
Dataset loaded from /tmp/tmpxg59md9s/merged_bio_data.nc and registered as 'merged_bio_data'
........
tests/bci_integration/test_bci_processing.py Computing rank from data with rank=None
    Using tolerance 0.018 (2.2e-16 eps * 64 dim * 1.3e+12  max singular value)
    Estimated rank (data): 64
    data: rank 64 computed from 64 data channels with 0 projectors
Reducing data rank from 64 -> 64
Estimating class=0 covariance using EMPIRICAL
Done.
Estimating class=1 covariance using EMPIRICAL
Done.
..Processing delta band. Input shape: (640, 1000)
Effective window size : 1.024 (s)
delta_power shape: (129, 64)
delta_wavelet shape: (64, 6)
Processing theta band. Input shape: (640, 1000)
Effective window size : 1.024 (s)
theta_power shape: (129, 64)
theta_wavelet shape: (64, 6)
Processing alpha band. Input shape: (640, 1000)
Effective window size : 1.024 (s)
alpha_power shape: (129, 64)
alpha_wavelet shape: (64, 6)
Processing beta band. Input shape: (640, 1000)
Effective window size : 1.024 (s)
beta_power shape: (129, 64)
beta_wavelet shape: (64, 6)
Processing gamma band. Input shape: (640, 1000)
Effective window size : 1.024 (s)
gamma_power shape: (129, 64)
gamma_wavelet shape: (64, 6)
Processing high_gamma band. Input shape: (640, 1000)
Effective window size : 1.024 (s)
high_gamma_power shape: (129, 64)
high_gamma_wavelet shape: (64, 6)
Shape of delta_power: (129, 64)
Shape of delta_wavelet: (64, 6)
Shape of theta_power: (129, 64)
Shape of theta_wavelet: (64, 6)
Shape of alpha_power: (129, 64)
Shape of alpha_wavelet: (64, 6)
Shape of beta_power: (129, 64)
Shape of beta_wavelet: (64, 6)
Shape of gamma_power: (129, 64)
Shape of gamma_wavelet: (64, 6)
Shape of high_gamma_power: (129, 64)
Shape of high_gamma_wavelet: (64, 6)
.Computing rank from data with rank=None
    Using tolerance 0.018 (2.2e-16 eps * 64 dim * 1.3e+12  max singular value)
    Estimated rank (data): 64
    data: rank 64 computed from 64 data channels with 0 projectors
Reducing data rank from 64 -> 64
Estimating class=0 covariance using EMPIRICAL
Done.
Estimating class=1 covariance using EMPIRICAL
Done.
Processing delta band. Input shape: (40, 64)
Effective window size : 1.024 (s)
delta_power shape: (129, 64)
delta_wavelet shape: (64, 6)
Processing theta band. Input shape: (40, 64)
Effective window size : 1.024 (s)
theta_power shape: (129, 64)
theta_wavelet shape: (64, 6)
Processing alpha band. Input shape: (40, 64)
Effective window size : 1.024 (s)
alpha_power shape: (129, 64)
alpha_wavelet shape: (64, 6)
Processing beta band. Input shape: (40, 64)
Effective window size : 1.024 (s)
beta_power shape: (129, 64)
beta_wavelet shape: (64, 6)
Processing gamma band. Input shape: (40, 64)
Effective window size : 1.024 (s)
gamma_power shape: (129, 64)
gamma_wavelet shape: (64, 6)
Processing high_gamma band. Input shape: (40, 64)
Effective window size : 1.024 (s)
high_gamma_power shape: (129, 64)
high_gamma_wavelet shape: (64, 6)
Shape of delta_power: (64, 129)
Shape of delta_wavelet: (64, 6)
Shape of theta_power: (64, 129)
Shape of theta_wavelet: (64, 6)
Shape of alpha_power: (64, 129)
Shape of alpha_wavelet: (64, 6)
Shape of beta_power: (64, 129)
Shape of beta_wavelet: (64, 6)
Shape of gamma_power: (64, 129)
Shape of gamma_wavelet: (64, 6)
Shape of high_gamma_power: (64, 129)
Shape of high_gamma_wavelet: (64, 6)
.
tests/bci_integration/test_neuro_data_integration.py .Computing rank from data with rank=None
    Using tolerance 0.018 (2.2e-16 eps * 64 dim * 1.3e+12  max singular value)
    Estimated rank (data): 64
    data: rank 64 computed from 64 data channels with 0 projectors
Reducing data rank from 64 -> 64
Estimating class=0 covariance using EMPIRICAL
Done.
Estimating class=1 covariance using EMPIRICAL
Done.
Processing delta band. Input shape: (40, 64)
Effective window size : 1.000 (s)
delta_power shape: (129, 64)
delta_wavelet shape: (64, 6)
Processing theta band. Input shape: (40, 64)
Effective window size : 1.000 (s)
theta_power shape: (129, 64)
theta_wavelet shape: (64, 6)
Processing alpha band. Input shape: (40, 64)
Effective window size : 1.000 (s)
alpha_power shape: (129, 64)
alpha_wavelet shape: (64, 6)
Processing beta band. Input shape: (40, 64)
Effective window size : 1.000 (s)
beta_power shape: (129, 64)
beta_wavelet shape: (64, 6)
Processing gamma band. Input shape: (40, 64)
Effective window size : 1.000 (s)
gamma_power shape: (129, 64)
gamma_wavelet shape: (64, 6)
Processing high_gamma band. Input shape: (40, 64)
Effective window size : 1.000 (s)
high_gamma_power shape: (129, 64)
high_gamma_wavelet shape: (64, 6)
..Computing rank from data with rank=None
    Using tolerance 0.018 (2.2e-16 eps * 64 dim * 1.3e+12  max singular value)
    Estimated rank (data): 64
    data: rank 64 computed from 64 data channels with 0 projectors
Reducing data rank from 64 -> 64
Estimating class=0 covariance using EMPIRICAL
Done.
Estimating class=1 covariance using EMPIRICAL
Done.
Processing delta band. Input shape: (40, 64)
Effective window size : 1.000 (s)
delta_power shape: (129, 64)
delta_wavelet shape: (64, 6)
Processing theta band. Input shape: (40, 64)
Effective window size : 1.000 (s)
theta_power shape: (129, 64)
theta_wavelet shape: (64, 6)
Processing alpha band. Input shape: (40, 64)
Effective window size : 1.000 (s)
alpha_power shape: (129, 64)
alpha_wavelet shape: (64, 6)
Processing beta band. Input shape: (40, 64)
Effective window size : 1.000 (s)
beta_power shape: (129, 64)
beta_wavelet shape: (64, 6)
Processing gamma band. Input shape: (40, 64)
Effective window size : 1.000 (s)
gamma_power shape: (129, 64)
gamma_wavelet shape: (64, 6)
Processing high_gamma band. Input shape: (40, 64)
Effective window size : 1.000 (s)
high_gamma_power shape: (129, 64)
high_gamma_wavelet shape: (64, 6)
.Computing rank from data with rank=None
    Using tolerance 0.018 (2.2e-16 eps * 64 dim * 1.3e+12  max singular value)
    Estimated rank (data): 64
    data: rank 64 computed from 64 data channels with 0 projectors
Reducing data rank from 64 -> 64
Estimating class=0 covariance using EMPIRICAL
Done.
Estimating class=1 covariance using EMPIRICAL
Done.
Processing delta band. Input shape: (40, 64)
Effective window size : 1.000 (s)
delta_power shape: (129, 64)
delta_wavelet shape: (64, 6)
Processing theta band. Input shape: (40, 64)
Effective window size : 1.000 (s)
theta_power shape: (129, 64)
theta_wavelet shape: (64, 6)
Processing alpha band. Input shape: (40, 64)
Effective window size : 1.000 (s)
alpha_power shape: (129, 64)
alpha_wavelet shape: (64, 6)
Processing beta band. Input shape: (40, 64)
Effective window size : 1.000 (s)
beta_power shape: (129, 64)
beta_wavelet shape: (64, 6)
Processing gamma band. Input shape: (40, 64)
Effective window size : 1.000 (s)
gamma_power shape: (129, 64)
gamma_wavelet shape: (64, 6)
Processing high_gamma band. Input shape: (40, 64)
Effective window size : 1.000 (s)
high_gamma_power shape: (129, 64)
high_gamma_wavelet shape: (64, 6)
.
tests/bci_integration/test_real_time_feedback.py .....Providing user feedback: -0.011621473252936183
Providing user feedback: 0.3423292897757681
Providing user feedback: -0.15594523745110184
Providing user feedback: -0.0630933462111236
Providing user feedback: -0.4229075500663163
Providing user feedback: 0.09869451739790337
Providing user feedback: -0.3366282755375898
Providing user feedback: -0.007093558048804983
Providing user feedback: -0.15259102895155288
Providing user feedback: 0.036991725849792885
.
tests/cognitive_architectures/test_advanced_thinking.py ........
tests/cognitive_architectures/test_cognitive_architectures.py .attended shape: (1, 10, 256)
attention_control shape before reshape: (1, 10, 64)
attention_control shape after reshape: (1, 10, 64)
attended shape: (1, 10, 256)
attention_control shape before reshape: (1, 10, 64)
attention_control shape after reshape: (1, 10, 64)
.attended shape: (1, 10, 256)
attention_control shape before reshape: (1, 10, 64)
attention_control shape after reshape: (1, 10, 64)
attended shape: (1, 10, 256)
attention_control shape before reshape: (1, 10, 64)
attention_control shape after reshape: (1, 10, 64)
.attended shape: (1, 10, 256)
attention_control shape before reshape: (1, 10, 64)
attention_control shape after reshape: (1, 10, 64)
attended shape: (1, 10, 256)
attention_control shape before reshape: (1, 10, 64)
attention_control shape after reshape: (1, 10, 64)
.....
tests/edge_ai/test_edge_ai_optimization.py ..Initial learning rate: 0.001
Initial model state: odict_keys(['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias'])
Test data shape: torch.Size([100, 10]), device: cpu
Performance after first evaluation: 0.0
Learning rate after first evaluation: 0.001089
Model state after first evaluation: odict_keys(['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias'])
Performance after second evaluation: 0.0
Learning rate after second evaluation: 0.0009703
Model state after second evaluation: odict_keys(['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias'])
Optimizer performance: 0.17568012448988546, Evaluated accuracy: 0.0
Simulated performance 1: 0.17361174301762033
Simulated performance 2: 0.17361174301762033
.....Initial learning rate: 0.001, Initial performance: 0.5
Optimizer initialized: True
Before self-heal - Learning rate: 0.001, Performance: 0.5
Performance history before self-heal: []
LEARNING_RATE_ADJUSTMENT: 0.1
After self-heal - Learning rate: 0.001, Performance: 0.4898746207530813
Performance history after self-heal: [0.4987454011884736, 0.49748765434059145, 0.4962267515573082, 0.49496268491975826, 0.493695446489206, 0.4924250283069959, 0.49115142239450277, 0.4898746207530813]
Learning rate history: [0.0007774538386783284]
EdgeAIOptimization learning rate: 0.001
Model optimizer learning rate: 0.0007774538386783284
....
tests/edge_ai/test_neuromorphic_computing.py ........
tests/scientific_domains/bioinformatics/test_bioinformatics_integration.py .....
tests/scientific_domains/bioinformatics/test_ete_integration.py ......
tests/scientific_domains/bioinformatics/test_scikit_bio_integration.py ......
tests/scientific_domains/biology/test_synthetic_biology_insights.py ....s
tests/scientific_domains/test_alphafold_integration.py s...........
tests/scientific_domains/test_art_integration.py .....
tests/scientific_domains/test_google_integration.py ...s..
tests/scientific_domains/test_math_solvers.py ..Unexpected warning in BFGS: Unknown solver options: ftol, maxls
Context: func=func, initial_guess=[0.9, 2.4]
Result: success=False, message=Desired error not necessarily achieved due to precision loss.
Function value at result: 1.1102188887259459e-16
Number of iterations: 2
Trying next method.
Optimization successful with method L-BFGS-B
Result: success=True, message=CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH
....
tests/scientific_domains/test_protein_development.py ........
tests/scientific_domains/test_transformers.py .s.s...
tests/scientific_domains/test_xarray_integration.py ...Dataset 'test_dataset' saved to /path/to/save/dataset.nc
.
tests/test_agentic_behavior.py ............
tests/test_core_components.py .................
tests/test_generative_ai.py ...Parsed expression: 120
Expanded expression: 120
Raw solution: []
Complex solutions: []
Final solution: [0.+0.j 0.-0.j]
.....conscious_state shape: (32, 10)
loss shape: ()
feedback shape: (32, 10)
conscious_state shape: (32, 10)
loss shape: ()
feedback shape: (32, 10)
conscious_state shape: (32, 10)
loss shape: ()
feedback shape: (32, 10)
conscious_state shape: (32, 10)
loss shape: ()
feedback shape: (32, 10)
conscious_state shape: (32, 10)
loss shape: ()
feedback shape: (32, 10)
conscious_state shape: (32, 10)
loss shape: ()
feedback shape: (32, 10)
..Parsed expression: x - 5
Expanded expression: x - 5
Raw solution: [5]
Complex solutions: [(5+0j)]
Parsed expression: x**2 + 1
Expanded expression: x**2 + 1
Raw solution: [-I, I]
Complex solutions: [-1j, 1j]
Final solution: [0.+1.j 0.-1.j]
Parsed expression: x**2 - 4
Expanded expression: x**2 - 4
Raw solution: [-2, 2]
Complex solutions: [(-2+0j), (2+0j)]
Final solution: [ 2.+0.j -2.+0.j]
Parsed expression: x**3 - x
Expanded expression: x**3 - x
Raw solution: [-1, 0, 1]
Complex solutions: [(-1+0j), 0j, (1+0j)]
Final solution: [0.+0.j 1.+0.j]
Parsed expression: 2*x + 3
Expanded expression: 2*x + 3
Raw solution: [-3/2]
Complex solutions: [(-1.5+0j)]
Parsed expression: x**2 + 2*x + 1
Expanded expression: x**2 + 2*x + 1
Raw solution: [-1]
Complex solutions: [(-1+0j)]
Parsed expression: x + y - 5
Expanded expression: x + y - 5
Raw solution: [5 - y]
Parsed expression: sin(x) - 0.5
Expanded expression: sin(x) - 0.5
Parsed expression: x - 5
Expanded expression: x - 5
Raw solution: [5]
Complex solutions: [(5+0j)]
Parsed expression: x**2 + 1
Expanded expression: x**2 + 1
Raw solution: [-I, I]
Complex solutions: [-1j, 1j]
Final solution: [0.+1.j 0.-1.j]
Parsed expression: x**2 - 4
Expanded expression: x**2 - 4
Raw solution: [-2, 2]
Complex solutions: [(-2+0j), (2+0j)]
Final solution: [ 2.+0.j -2.+0.j]
Parsed expression: x**3 - x
Expanded expression: x**3 - x
Raw solution: [-1, 0, 1]
Complex solutions: [(-1+0j), 0j, (1+0j)]
Final solution: [0.+0.j 1.+0.j]
Parsed expression: 2*x + 3
Expanded expression: 2*x + 3
Raw solution: [-3/2]
Complex solutions: [(-1.5+0j)]
Parsed expression: x**2 + 2*x + 1
Expanded expression: x**2 + 2*x + 1
Raw solution: [-1]
Complex solutions: [(-1+0j)]
Parsed expression: x**3 - 6*x**2 + 11*x - 6
Expanded expression: x**3 - 6*x**2 + 11*x - 6
Raw solution: [1, 2, 3]
Complex solutions: [(1+0j), (2+0j), (3+0j)]
Final solution: [1.+0.j 2.+0.j]
Parsed expression: x**2 + 1
Expanded expression: x**2 + 1
Raw solution: [-I, I]
Complex solutions: [-1j, 1j]
Final solution: [0.+1.j 0.-1.j]
.Parsed expression: 2*x - 4
Expanded expression: 2*x - 4
Raw solution: [2]
Complex solutions: [(2+0j)]
Parsed expression: x**2 + 1
Expanded expression: x**2 + 1
Raw solution: [-I, I]
Complex solutions: [-1j, 1j]
Final solution: [0.+1.j 0.-1.j]
..........
tests/test_quantum_models.py ......
tests/test_quantum_module.py .......

=============================== warnings summary ===============================
../.pyenv/versions/3.9.16/lib/python3.9/site-packages/Bio/Data/SCOPData.py:18
  /home/ubuntu/.pyenv/versions/3.9.16/lib/python3.9/site-packages/Bio/Data/SCOPData.py:18: BiopythonDeprecationWarning: The 'Bio.Data.SCOPData' module will be deprecated in a future release of Biopython in favor of 'Bio.Data.PDBData.
    warnings.warn(

tests/advanced_models/test_advanced_time_series_analysis.py::test_analyze_valid_methods[sarima]
tests/advanced_models/test_advanced_time_series_analysis.py::test_sarima_forecast
tests/advanced_models/test_advanced_time_series_analysis.py::test_sarima_forecast
tests/advanced_models/test_advanced_time_series_analysis.py::test_analyze_different_configurations[sarima-order2-seasonal_order2-data2]
tests/advanced_models/test_advanced_time_series_analysis.py::test_analyze_different_configurations[sarima-order3-seasonal_order3-data3]
  /home/ubuntu/NeuroFlex/NeuroFlex/advanced_models/advanced_time_series_analysis.py:235: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.
    warnings.warn("Non-invertible starting MA parameters found. Using zeros as starting parameters.", UserWarning)

tests/advanced_models/test_advanced_time_series_analysis.py::test_analyze_valid_methods[sarima]
tests/advanced_models/test_advanced_time_series_analysis.py::test_sarima_forecast
tests/advanced_models/test_advanced_time_series_analysis.py::test_sarima_forecast
tests/advanced_models/test_advanced_time_series_analysis.py::test_analyze_different_configurations[sarima-order2-seasonal_order2-data2]
tests/advanced_models/test_advanced_time_series_analysis.py::test_analyze_different_configurations[sarima-order3-seasonal_order3-data3]
  /home/ubuntu/.pyenv/versions/3.9.16/lib/python3.9/site-packages/statsmodels/tsa/statespace/mlemodel.py:3016: RuntimeWarning: invalid value encountered in divide
    return self.params / self.bse

tests/advanced_models/test_advanced_time_series_analysis.py::test_analyze_valid_methods[sarima]
tests/advanced_models/test_advanced_time_series_analysis.py::test_sarima_forecast
tests/advanced_models/test_advanced_time_series_analysis.py::test_sarima_forecast
tests/advanced_models/test_advanced_time_series_analysis.py::test_analyze_different_configurations[sarima-order2-seasonal_order2-data2]
tests/advanced_models/test_advanced_time_series_analysis.py::test_analyze_different_configurations[sarima-order3-seasonal_order3-data3]
  /home/ubuntu/.pyenv/versions/3.9.16/lib/python3.9/site-packages/statsmodels/tsa/stattools.py:1431: RuntimeWarning: invalid value encountered in divide
    test_statistic = numer_squared_sum / denom_squared_sum

tests/advanced_models/test_advanced_time_series_analysis.py::test_analyze_valid_methods[sarima]
tests/advanced_models/test_advanced_time_series_analysis.py::test_sarima_forecast
tests/advanced_models/test_advanced_time_series_analysis.py::test_sarima_forecast
tests/advanced_models/test_advanced_time_series_analysis.py::test_analyze_different_configurations[sarima-order2-seasonal_order2-data2]
tests/advanced_models/test_advanced_time_series_analysis.py::test_analyze_different_configurations[sarima-order3-seasonal_order3-data3]
  /home/ubuntu/.pyenv/versions/3.9.16/lib/python3.9/site-packages/statsmodels/tsa/stattools.py:702: RuntimeWarning: invalid value encountered in divide
    acf = avf[: nlags + 1] / avf[0]

tests/advanced_models/test_advanced_time_series_analysis.py::test_analyze_different_configurations[sarima-order2-seasonal_order2-data2]
tests/advanced_models/test_advanced_time_series_analysis.py::test_analyze_different_configurations[sarima-order2-seasonal_order2-data2]
tests/advanced_models/test_advanced_time_series_analysis.py::test_analyze_different_configurations[sarima-order3-seasonal_order3-data3]
tests/advanced_models/test_advanced_time_series_analysis.py::test_analyze_different_configurations[sarima-order3-seasonal_order3-data3]
  /home/ubuntu/.pyenv/versions/3.9.16/lib/python3.9/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals
    warnings.warn("Maximum Likelihood optimization failed to "

tests/ai_ethics/test_ai_ethics.py::test_aif360_integration
  /home/ubuntu/.pyenv/versions/3.9.16/lib/python3.9/site-packages/aif360/metrics/utils.py:36: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison
    group_cond = np.logical_and(group_cond, X[:, index] == val)

tests/ai_ethics/test_ai_ethics.py::test_aif360_integration
  /home/ubuntu/.pyenv/versions/3.9.16/lib/python3.9/site-packages/aif360/metrics/binary_label_dataset_metric.py:105: RuntimeWarning: invalid value encountered in double_scalars
    return (self.num_positives(privileged=privileged)

tests/ai_ethics/test_ai_ethics.py::test_aif360_integration
  /home/ubuntu/.pyenv/versions/3.9.16/lib/python3.9/site-packages/aif360/algorithms/preprocessing/reweighing.py:66: RuntimeWarning: invalid value encountered in double_scalars
    self.w_p_fav = n_fav*n_p / (n*n_p_fav)

tests/ai_ethics/test_ai_ethics.py::test_aif360_integration
  /home/ubuntu/.pyenv/versions/3.9.16/lib/python3.9/site-packages/aif360/algorithms/preprocessing/reweighing.py:67: RuntimeWarning: invalid value encountered in double_scalars
    self.w_p_unfav = n_unfav*n_p / (n*n_p_unfav)

tests/ai_ethics/test_ai_ethics.py::test_aif360_integration
  /home/ubuntu/.pyenv/versions/3.9.16/lib/python3.9/site-packages/aif360/algorithms/preprocessing/reweighing.py:68: RuntimeWarning: invalid value encountered in double_scalars
    self.w_up_fav = n_fav*n_up / (n*n_up_fav)

tests/ai_ethics/test_ai_ethics.py::test_aif360_integration
  /home/ubuntu/.pyenv/versions/3.9.16/lib/python3.9/site-packages/aif360/algorithms/preprocessing/reweighing.py:69: RuntimeWarning: invalid value encountered in double_scalars
    self.w_up_unfav = n_unfav*n_up / (n*n_up_unfav)

tests/ai_ethics/test_security_integration.py::TestSecurityIntegration::test_fairness_evaluation
tests/ai_ethics/test_security_integration.py::TestSecurityIntegration::test_model_health_check
tests/ai_ethics/test_security_integration.py::TestSecurityIntegration::test_perform_security_check
tests/ai_ethics/test_security_integration.py::TestSecurityIntegration::test_secure_action
tests/ai_ethics/test_security_integration.py::TestSecurityIntegration::test_security_integration_in_training
tests/ai_ethics/test_security_integration.py::TestSecurityIntegration::test_threat_detection_and_mitigation
  /home/ubuntu/.pyenv/versions/3.9.16/lib/python3.9/site-packages/Bio/Seq.py:3482: BiopythonWarning: Partial codon, len(sequence) not a multiple of three. Explicitly trim the sequence or add trailing N before translation. This may become an error in future.
    warnings.warn(

tests/ai_ethics/test_security_integration.py: 17 warnings
tests/scientific_domains/bioinformatics/test_scikit_bio_integration.py: 1 warning
  /home/ubuntu/.pyenv/versions/3.9.16/lib/python3.9/site-packages/skbio/alignment/_pairwise.py:595: EfficiencyWarning: You're using skbio's python implementation of Needleman-Wunsch alignment. This is known to be very slow (e.g., thousands of times slower than a native C implementation). We'll be adding a faster version soon (see https://github.com/biocore/scikit-bio/issues/254 to track progress on this).
    warn("You're using skbio's python implementation of Needleman-Wunsch "

tests/ai_ethics/test_security_integration.py::TestAdvancedSecurityAgentIntegration::test_generate_security_report_with_bioinformatics
tests/scientific_domains/test_art_integration.py::TestARTIntegration::test_adversarial_training
tests/scientific_domains/test_art_integration.py::TestARTIntegration::test_evaluate_robustness
tests/scientific_domains/test_art_integration.py::TestARTIntegration::test_generate_adversarial_examples
  /home/ubuntu/.pyenv/versions/3.9.16/lib/python3.9/site-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
    updates=self.state_updates,

tests/bci_integration/test_bci_processing.py::test_preprocess
tests/bci_integration/test_bci_processing.py::test_process
tests/bci_integration/test_neuro_data_integration.py::test_integrate_eeg_data
tests/bci_integration/test_neuro_data_integration.py::test_get_integrated_data
tests/bci_integration/test_neuro_data_integration.py::test_perform_multimodal_analysis
  /home/ubuntu/.pyenv/versions/3.9.16/lib/python3.9/site-packages/sklearn/decomposition/_fastica.py:488: FutureWarning: From version 1.3 whiten='unit-variance' will be used by default.
    warnings.warn(

tests/bci_integration/test_bci_processing.py::test_preprocess
tests/bci_integration/test_bci_processing.py::test_process
tests/bci_integration/test_neuro_data_integration.py::test_integrate_eeg_data
tests/bci_integration/test_neuro_data_integration.py::test_get_integrated_data
tests/bci_integration/test_neuro_data_integration.py::test_perform_multimodal_analysis
  /home/ubuntu/.pyenv/versions/3.9.16/lib/python3.9/site-packages/sklearn/decomposition/_fastica.py:120: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
    warnings.warn(

tests/bci_integration/test_bci_processing.py::test_process
tests/bci_integration/test_neuro_data_integration.py::test_integrate_eeg_data
tests/bci_integration/test_neuro_data_integration.py::test_get_integrated_data
tests/bci_integration/test_neuro_data_integration.py::test_perform_multimodal_analysis
  /home/ubuntu/.pyenv/versions/3.9.16/lib/python3.9/site-packages/pywt/_multilevel.py:43: UserWarning: Level value of 5 is too high: all coefficients will experience boundary effects.
    warnings.warn(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========== 268 passed, 6 skipped, 73 warnings in 137.41s (0:02:17) ============

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
    5      1     21      1     0     0   5.744D+14  -0.000D+00
  F =  -0.0000000000000000     

ABNORMAL_TERMINATION_IN_LNSRCH                              
